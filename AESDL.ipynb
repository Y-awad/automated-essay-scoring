{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26f13b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core data handling\n",
    "import pandas as pd  # Load and manipulate TSV dataset (training_set_rel3.tsv)\n",
    "import numpy as np  # Numerical operations for feature arrays and score normalization\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk  # Stopwords, lemmatization, tokenization, and spelling correction\n",
    "from nltk.corpus import stopwords, wordnet, words  # Resources for stopwords, lemmatization, and spelling\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  # Sentence and word tokenization\n",
    "from nltk.metrics.distance import edit_distance  # Edit distance for spelling correction\n",
    "import re  # Regular expressions for cleaning ASAP tokens (e.g., @SOURCE1) and contractions\n",
    "import string  # Punctuation removal\n",
    "\n",
    "\n",
    "# Model-related (PyTorch)\n",
    "import torch  # PyTorch for model and tensor conversion of preprocessed data\n",
    "from torch.nn.utils.rnn import pad_sequence  # Pad sequences for LSTM input\n",
    "\n",
    "# Evaluation and splitting\n",
    "from sklearn.model_selection import train_test_split  # Stratified train-test splits by essay_set\n",
    "from sklearn.metrics import cohen_kappa_score  # Quadratic Weighted Kappa for evaluation\n",
    "\n",
    "# Optional: Visualization\n",
    "import matplotlib.pyplot as plt  # Plot score distributions to identify imbalances\n",
    "import seaborn as sns  # Enhanced visualization for score analysis\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b599428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_in_sets(data):\n",
    "    essay_sets = []\n",
    "    min_scores = []\n",
    "    max_scores = []\n",
    "    for s in range(1, 9):\n",
    "        essay_set = data[data[\"essay_set\"] == s].copy()  # Avoid modifying original\n",
    "        # Drop irrelevant columns (specific to each set)\n",
    "        columns_to_drop = [\"rater1_domain1\", \"rater2_domain1\", \"rater3_domain1\"]\n",
    "        if s != 2:\n",
    "            columns_to_drop.extend([\"rater1_domain2\", \"rater2_domain2\", \"domain2_score\"])\n",
    "        if s not in [7, 8]:\n",
    "            columns_to_drop.extend([col for col in data.columns if \"trait\" in col])\n",
    "        essay_set = essay_set.drop(columns=[col for col in columns_to_drop if col in essay_set.columns])\n",
    "        n, d = essay_set.shape\n",
    "        set_scores = essay_set[\"domain1_score\"]\n",
    "        print(f\"Set {s}: Essays = {n}, Attributes = {d}\")\n",
    "        min_scores.append(set_scores.min())\n",
    "        max_scores.append(set_scores.max())\n",
    "        essay_sets.append(essay_set)\n",
    "    return essay_sets, min_scores, max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88263941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1: Essays = 1783, Attributes = 4\n",
      "Set 2: Essays = 1800, Attributes = 7\n",
      "Set 3: Essays = 1726, Attributes = 4\n",
      "Set 4: Essays = 1770, Attributes = 4\n",
      "Set 5: Essays = 1805, Attributes = 4\n",
      "Set 6: Essays = 1800, Attributes = 4\n",
      "Set 7: Essays = 1569, Attributes = 22\n",
      "Set 8: Essays = 723, Attributes = 22\n",
      "Score ranges: [(2, 12), (1, 6), (0, 3), (0, 3), (0, 4), (0, 4), (2, 24), (10, 60)]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_path = \"training_set_rel3.tsv\"\n",
    "data = pd.read_csv(dataset_path, sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Split into sets and get score ranges\n",
    "essay_sets, min_scores, max_scores = split_in_sets(data)\n",
    "set1, set2, set3, set4, set5, set6, set7, set8 = tuple(essay_sets)\n",
    "sets = [set1, set2, set3, set4, set5, set6, set7, set8]\n",
    "print(\"Score ranges:\", list(zip(min_scores, max_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d2b893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced text cleaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_list = set(words.words())\n",
    "lemmatizer = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2098d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "def enhanced_clean_essay(essay, ner_tokens):\n",
    "    essay = essay.lower()\n",
    "    # Handle contractions\n",
    "    contractions = {\"can't\": \"cannot\", \"don't\": \"do not\", \"won't\": \"will not\", \"it's\": \"it is\"}\n",
    "    for contr, expand in contractions.items():\n",
    "        essay = re.sub(r\"\\b\" + contr + r\"\\b\", expand, essay)\n",
    "    # Replace all NER tokens with \"entity\"\n",
    "    for token in ner_tokens:\n",
    "        essay = re.sub(r'\\b' + re.escape(token) + r'\\b', 'entity', essay)\n",
    "    # Remove punctuation and numbers\n",
    "    essay = essay.translate(str.maketrans('', '', string.punctuation))\n",
    "    essay = re.sub(r'\\d+', '', essay)\n",
    "    # Spelling correction with pyspellchecker\n",
    "    #words = essay.split()\n",
    "    #corrected = [spell.correction(word) if spell.correction(word) else word for word in words]\n",
    "    #essay = ' '.join(corrected)\n",
    "    # Remove stopwords\n",
    "    essay = ' '.join(word for word in essay.split() if word not in stop_words)\n",
    "    # Lemmatize\n",
    "    essay = ' '.join(lemmatizer.lemmatize(word) for word in essay.split())\n",
    "    return essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deb10948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 168 unique NER tokens: {'@PERCENT1', '@DATE3', '@TIME1', '@PERSON3', '@CAPS48', '@CAPS38', '@NUM2', '@CAPS33', '@LOCATION2M', '@CAPS19', '@CAPS40', '@NUM7', '@CAPS77', '@NUM4', '@CAPS18', '@MONEY1', '@CAPS37', '@CAPS8', '@CAPS44', '@CAPS14', '@CAPS39', '@NUM1king', '@ORGANIZATION3', '@MONTH1', '@CAPS27', '@CAPS71', '@CAPS62', '@DATE2', '@PERCENT4', '@CAPS15', '@NUM1th', '@DATE4', '@NUM10', '@CAPS29', '@NUM5', '@PERCENT7', '@CAPS23', '@CAPS2', '@TIME3', '@TIME4', '@ORGANIZATION1n', '@LOCATION1L', '@CAPS12', '@PERSON7', '@NUM1o', '@DATE6', '@CAPS74', '@CAPS36', '@STATE1', '@CAPS63', '@PERSON4', '@LOCATION1â', '@CAPS35', '@PERCENT5', '@LOCATION12', '@NUM13', '@PERCENT2', '@CAPS30', '@CAPS11â', '@CAPS25', '@CAPS6', '@MONEY2', '@NUM6', '@CAPS9', '@DATE5', '@CAPS52', '@CAPS49', '@LOCATION4', '@CAPS13', '@NUM1at', '@CAPS46', '@TIME1pm', '@CAPS76', '@PERCENT3', '@LOCATION4from', '@LOCATION3', '@NUM2th', '@NUM1', '@CAPS43', '@LOCATION6', '@CAPS53', '@LOCATION8', '@CAPS3', '@CAPS4', '@CAPS20', '@DR1', '@LOCATION7', '@CAPS2â', '@CAPS21', '@CAPS7', '@CAPS26', '@PERSON5', '@NUM1er', '@PERSON6', '@TIME2', '@CAPS55', '@PERCENT1of', '@PERCENT6', '@CAPS22', '@CAPS66', '@ORGANIZATION5', '@NUM5sucsess', '@CAPS12â', '@ORGANIZATION6', '@CAPS54', '@LOCATION11', '@TIME1AM', '@NUM1ed', '@CAPS28', '@LOCATION4R', '@CITY1', '@CAPS34', '@PERSON1', '@ORGANIZATION2', '@LOCATION1A', '@PERSON2hemlock', '@CAPS61', '@CAPS73', '@CAPS16â', '@LOCATION1', '@CAPS16', '@CAPS32', '@NUM1k', '@NUM12', '@PERSON8', '@ORGANIZATION1', '@PERSON1â', '@CAPS5', '@NUM8', '@MONEY4', '@LOCATION10', '@TIME1am', '@LOCATION9', '@MONEY3', '@CAPS31', '@NUM2scalier', '@LOCATION5', '@EMAIL1', '@PERSON2â', '@PERSON9', '@TIME2So', '@NUM9', '@CAPS1', '@NUM1r', '@PERSON2', '@CAPS3â', '@ORGANIZATION4', '@CAPS17', '@CAPS11', '@DR2', '@CAPS24', '@CAPS4â', '@DATE1', '@PERSON3â', '@LOCATION2', '@CAPS10', '@LOCATION4A', '@CAPS67', '@PERSON2C', '@NUM2nd', '@CAPS41', '@CAPS56', '@PERSON4hawk', '@CAPS59', '@CAPS17â', '@NUM11', '@NUM1le', '@NUM3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12976/12976 [00:53<00:00, 241.99it/s]\n",
      "100%|██████████| 1783/1783 [00:11<00:00, 149.90it/s]\n",
      "100%|██████████| 1800/1800 [00:12<00:00, 145.91it/s]\n",
      "100%|██████████| 1726/1726 [00:03<00:00, 470.29it/s]\n",
      "100%|██████████| 1770/1770 [00:03<00:00, 534.35it/s]\n",
      "100%|██████████| 1805/1805 [00:04<00:00, 410.61it/s]\n",
      "100%|██████████| 1800/1800 [00:05<00:00, 326.79it/s]\n",
      "100%|██████████| 1569/1569 [00:04<00:00, 324.09it/s]\n",
      "100%|██████████| 723/723 [00:07<00:00, 95.03it/s] \n"
     ]
    }
   ],
   "source": [
    "# Optimized NER token list\n",
    "all_essays = ' '.join(data['essay'])\n",
    "ner = set(re.findall(r'@\\w+\\d*', all_essays))  # Extract unique tokens\n",
    "print(f\"Found {len(ner)} unique NER tokens: {ner}\")\n",
    "\n",
    "# Apply cleaning with progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "data['cleaned_essay'] = data['essay'].progress_apply(lambda x: enhanced_clean_essay(x, ner))\n",
    "for i, essay_set in enumerate(sets):\n",
    "    essay_set['cleaned_essay'] = essay_set['essay'].progress_apply(lambda x: enhanced_clean_essay(x, ner))\n",
    "    sets[i] = essay_set\n",
    "\n",
    "# Save cleaned data\n",
    "data.to_csv('cleaned_data.csv', index=False)\n",
    "for i, essay_set in enumerate(sets):\n",
    "    essay_set.to_csv(f'cleaned_set{i+1}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "168c705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   essay_set                                              essay  \\\n",
      "0          1  Dear local newspaper, I think effects computer...   \n",
      "1          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4          1  Dear @LOCATION1, I know having computers has a...   \n",
      "\n",
      "                                       cleaned_essay  domain1_score  \\\n",
      "0  dear local newspaper think effect computer peo...              8   \n",
      "1  dear cap cap believe using computer benefit u ...              9   \n",
      "2  dear cap cap cap people use computer everyone ...              7   \n",
      "3  dear local newspaper cap found many expert say...             10   \n",
      "4  dear location know computer positive effect pe...              8   \n",
      "\n",
      "   normalized_score  \n",
      "0               0.6  \n",
      "1               0.7  \n",
      "2               0.5  \n",
      "3               0.8  \n",
      "4               0.6  \n"
     ]
    }
   ],
   "source": [
    "# Normalize scores\n",
    "def normalize_score(row):\n",
    "    set_id = row['essay_set'] - 1  # Adjust for 0-based indexing\n",
    "    score = row['domain1_score']\n",
    "    return (score - min_scores[set_id]) / (max_scores[set_id] - min_scores[set_id])\n",
    "\n",
    "data['normalized_score'] = data.apply(normalize_score, axis=1)\n",
    "for i, essay_set in enumerate(sets):\n",
    "    essay_set['normalized_score'] = essay_set.apply(normalize_score, axis=1)\n",
    "    sets[i] = essay_set\n",
    "\n",
    "# Display sample\n",
    "print(data[['essay_set', 'essay', 'cleaned_essay', 'domain1_score', 'normalized_score']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
