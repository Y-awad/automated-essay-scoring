{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26f13b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ecc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core data handling\n",
    "import pandas as pd  # Load and manipulate TSV dataset (training_set_rel3.tsv)\n",
    "import numpy as np  # Numerical operations for feature arrays and score normalization\n",
    "\n",
    "# Text preprocessing\n",
    "import nltk  # Stopwords, lemmatization, tokenization, and spelling correction\n",
    "from nltk.corpus import stopwords, wordnet, words  # Resources for stopwords, lemmatization, and spelling\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  # Sentence and word tokenization\n",
    "from nltk.metrics.distance import edit_distance  # Edit distance for spelling correction\n",
    "import re  # Regular expressions for cleaning ASAP tokens (e.g., @SOURCE1) and contractions\n",
    "import string  # Punctuation removal\n",
    "\n",
    "\n",
    "# Model-related (PyTorch)\n",
    "import torch  # PyTorch for model and tensor conversion of preprocessed data\n",
    "from torch.nn.utils.rnn import pad_sequence  # Pad sequences for LSTM input\n",
    "\n",
    "# Evaluation and splitting\n",
    "from sklearn.model_selection import train_test_split  # Stratified train-test splits by essay_set\n",
    "from sklearn.metrics import cohen_kappa_score  # Quadratic Weighted Kappa for evaluation\n",
    "\n",
    "# Optional: Visualization\n",
    "import matplotlib.pyplot as plt  # Plot score distributions to identify imbalances\n",
    "import seaborn as sns  # Enhanced visualization for score analysis\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b599428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_in_sets(data):\n",
    "    essay_sets = []\n",
    "    min_scores = []\n",
    "    max_scores = []\n",
    "    for s in range(1, 9):\n",
    "        essay_set = data[data[\"essay_set\"] == s].copy()  # Avoid modifying original\n",
    "\n",
    "        # Drop irrelevant columns (specific to each set)\n",
    "        columns_to_drop = [\"rater1_domain1\", \"rater2_domain1\", \"rater3_domain1\"]\n",
    "        if s != 2:\n",
    "            columns_to_drop.extend([\"rater1_domain2\", \"rater2_domain2\", \"domain2_score\"])\n",
    "        if s not in [7, 8]:\n",
    "            columns_to_drop.extend([col for col in data.columns if \"trait\" in col])\n",
    "        essay_set = essay_set.drop(columns=[col for col in columns_to_drop if col in essay_set.columns])\n",
    "\n",
    "        # Keep only cleaned essays (drop raw 'essay')\n",
    "        if \"essay\" in essay_set.columns:\n",
    "            essay_set = essay_set.drop(columns=[\"essay\"])\n",
    "        essay_set = essay_set.rename(columns={\"essay_clean\": \"essay\"})\n",
    "\n",
    "        # Stats\n",
    "        n, d = essay_set.shape\n",
    "        set_scores = essay_set[\"domain1_score\"]\n",
    "        print(f\"Set {s}: Essays = {n}, Attributes = {d}\")\n",
    "\n",
    "        min_scores.append(set_scores.min())\n",
    "        max_scores.append(set_scores.max())\n",
    "        essay_sets.append(essay_set)\n",
    "\n",
    "    return essay_sets, min_scores, max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "88263941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def bert_friendly_clean(text, normalize_ner=True):\n",
    "    # 1. Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # 2. Normalize spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # 3. Handle contractions\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\", \"won't\": \"will not\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\",\n",
    "        \"'ll\": \" will\", \"'t\": \" not\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    for contr, expand in contractions.items():\n",
    "        text = re.sub(contr, expand, text)\n",
    "\n",
    "    # 4. Handle NER tokens (e.g. @PERSON1 → [ENTITY])\n",
    "    if normalize_ner:\n",
    "        text = re.sub(r\"@\\w+\\d*\", \"[ENTITY]\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9d2b893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning essays for BERT...\n",
      "✅ Cleaned dataset saved to training_set_rel3_cleaned.tsv\n",
      "Set 1: Essays = 1783, Attributes = 4\n",
      "Set 2: Essays = 1800, Attributes = 7\n",
      "Set 3: Essays = 1726, Attributes = 4\n",
      "Set 4: Essays = 1770, Attributes = 4\n",
      "Set 5: Essays = 1805, Attributes = 4\n",
      "Set 6: Essays = 1800, Attributes = 4\n",
      "Set 7: Essays = 1569, Attributes = 22\n",
      "Set 8: Essays = 723, Attributes = 22\n",
      "Score ranges: [(2, 12), (1, 6), (0, 3), (0, 3), (0, 4), (0, 4), (2, 24), (10, 60)]\n",
      "\n",
      "Original essay sample:\n",
      "Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is alw\n",
      "\n",
      "Cleaned essay sample:\n",
      "Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is alw\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset_path = \"training_set_rel3.tsv\"\n",
    "data = pd.read_csv(dataset_path, sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Clean all essays upfront\n",
    "print(\"Cleaning essays for BERT...\")\n",
    "data[\"essay_clean\"] = data[\"essay\"].apply(lambda x: bert_friendly_clean(str(x)))\n",
    "\n",
    "# Save cleaned dataset to disk\n",
    "cleaned_path = \"training_set_rel3_cleaned.tsv\"\n",
    "data.to_csv(cleaned_path, sep=\"\\t\", index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Cleaned dataset saved to {cleaned_path}\")\n",
    "\n",
    "# Split into sets and get score ranges\n",
    "essay_sets, min_scores, max_scores = split_in_sets(data)\n",
    "\n",
    "# Unpack sets\n",
    "set1, set2, set3, set4, set5, set6, set7, set8 = tuple(essay_sets)\n",
    "sets = [set1, set2, set3, set4, set5, set6, set7, set8]\n",
    "\n",
    "print(\"Score ranges:\", list(zip(min_scores, max_scores)))\n",
    "\n",
    "# Quick check: raw vs clean\n",
    "print(\"\\nOriginal essay sample:\")\n",
    "print(data.loc[0, \"essay\"][:300])\n",
    "\n",
    "print(\"\\nCleaned essay sample:\")\n",
    "print(data.loc[0, \"essay_clean\"][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c203c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data[\"essay_clean\"].tolist()\n",
    "labels = data[\"domain1_score\"].values\n",
    "\n",
    "# Train/val/test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4a778426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [01:39<00:00,  2.84it/s]\n",
      "100%|██████████| 61/61 [00:22<00:00,  2.73it/s]\n",
      "100%|██████████| 61/61 [00:22<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load tokenizer & model (DistilBERT is lighter, you can swap with BertModel if you want)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "bert_model.eval()  # we’re only extracting features, not fine-tuning yet\n",
    "\n",
    "# Pick device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "def get_bert_embeddings(texts, max_len=128, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        encodings = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encodings[\"input_ids\"].to(device)\n",
    "        attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:,0,:]  # [CLS] token\n",
    "        all_embeddings.append(cls_embeddings.cpu())\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Get embeddings\n",
    "X_train_emb = get_bert_embeddings(X_train)\n",
    "X_val_emb   = get_bert_embeddings(X_val)\n",
    "X_test_emb  = get_bert_embeddings(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3b06c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------------\n",
    "# Model (BiLSTM 400 -> 128)\n",
    "# -------------------------\n",
    "class EssayBiLSTM(nn.Module):\n",
    "    def __init__(self, input_size=768, hidden_dim1=400, hidden_dim2=128, dropout=0.5, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_dim1,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            \n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=hidden_dim1 * (2 if bidirectional else 1),\n",
    "            hidden_size=hidden_dim2,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            \n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim2 * (2 if bidirectional else 1), 1)  # regression scalar\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, 768). Here seq_len = 1 if you use CLS embeddings only.\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, (h_n, _) = self.lstm2(out)\n",
    "        if self.bidirectional:\n",
    "            last_hidden = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            last_hidden = h_n[-1]\n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out)  # linear output; clamp later if needed\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6c33ec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb: torch.Size([32, 1, 768]) torch.float32\n",
      "yb: torch.Size([32]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ======================\n",
    "# Custom Dataset\n",
    "# ======================\n",
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        # Convert to numpy and squeeze extra dims\n",
    "        self.embeddings = np.array(embeddings).squeeze()\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each essay embedding -> (768,)\n",
    "        x = torch.tensor(self.embeddings[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        # Reshape for LSTM: (seq_len=1, input_size=768)\n",
    "        return x.unsqueeze(0), y\n",
    "\n",
    "# ======================\n",
    "# Create datasets\n",
    "# ======================\n",
    "train_dataset = EssayDataset(X_train_emb, y_train)\n",
    "val_dataset   = EssayDataset(X_val_emb, y_val)\n",
    "test_dataset  = EssayDataset(X_test_emb, y_test)\n",
    "\n",
    "# ======================\n",
    "# DataLoaders\n",
    "# ======================\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Quick shape check\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"xb:\", xb.shape, xb.dtype)\n",
    "print(\"yb:\", yb.shape, yb.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a49b64a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in train embeddings: False\n",
      "Infs in train embeddings: False\n"
     ]
    }
   ],
   "source": [
    "# 3. Check embeddings for NaNs/Infs\n",
    "import numpy as np\n",
    "\n",
    "print(\"NaNs in train embeddings:\", np.isnan(X_train_emb).any())\n",
    "print(\"Infs in train embeddings:\", np.isinf(X_train_emb).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "90fedee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ======================\n",
    "# Training loop\n",
    "# ======================\n",
    "def train_model(model, train_loader, val_loader, epochs=5, lr=1e-3, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()   # regression -> MSE\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb).squeeze()   # shape [batch]\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                outputs = model(xb).squeeze()\n",
    "                val_loss += criterion(outputs, yb).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ======================\n",
    "# Evaluation\n",
    "# ======================\n",
    "def evaluate_model(model, test_loader, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    criterion = nn.MSELoss()\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            outputs = model(xb).squeeze()\n",
    "            preds.extend(outputs.cpu().numpy())\n",
    "            true.extend(yb.cpu().numpy())\n",
    "            test_loss += criterion(outputs, yb).item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Test MSE: {test_loss:.4f}\")\n",
    "    return np.array(preds), np.array(true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1d642bbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m EssayBiLSTM()\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m preds, true \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[130], line 9\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_model\u001b[39m(model, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()   \u001b[38;5;66;03m# regression -> MSE\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mRMSprop(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:285\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 285\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1324\u001b[0m             device,\n\u001b[0;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1326\u001b[0m             non_blocking,\n\u001b[0;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1328\u001b[0m         )\n\u001b[1;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = EssayBiLSTM()\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=5, device=device)\n",
    "\n",
    "preds, true = evaluate_model(model, test_loader, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
