{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2667,"databundleVersionId":29898,"sourceType":"competition"},{"sourceId":13041988,"sourceType":"datasetVersion","datasetId":8258500}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning for Automated Essay Scoring","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import cohen_kappa_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom transformers import AutoTokenizer, AutoModel\n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T20:43:55.935593Z","iopub.execute_input":"2025-09-12T20:43:55.935750Z","iopub.status.idle":"2025-09-12T20:44:10.219686Z","shell.execute_reply.started":"2025-09-12T20:43:55.935735Z","shell.execute_reply":"2025-09-12T20:44:10.218808Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\ndef normalize_scores(scores, set_id, min_scores, max_scores):\n    mi = min_scores[set_id-1]\n    ma = max_scores[set_id-1]\n    return (scores - mi) / (ma - mi)\n\ndef denormalize_scores(scores_norm, set_id, min_scores, max_scores):\n    mi = min_scores[set_id-1]\n    ma = max_scores[set_id-1]\n    return scores_norm * (ma - mi) + mi","metadata":{"execution":{"iopub.status.busy":"2025-09-12T20:44:19.384456Z","iopub.execute_input":"2025-09-12T20:44:19.385256Z","iopub.status.idle":"2025-09-12T20:44:19.391209Z","shell.execute_reply.started":"2025-09-12T20:44:19.385220Z","shell.execute_reply":"2025-09-12T20:44:19.390405Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def get_encoder(model_name, unfreeze_last_n=0):\n    if model_name == \"roberta\":\n        name = \"roberta-base\"\n    elif model_name == \"bge\":\n        name = \"BAAI/bge-base-en\"\n    elif model_name == \"e5-base\":\n        name = \"intfloat/e5-base-v2\"\n    elif model_name == \"e5-large\":\n        name = \"intfloat/e5-large\"\n    elif model_name == \"qwen\":\n        name = \"Qwen/Qwen3-Embedding-0.6B\"   # qwen 0.6B close checkpoint\n    elif model_name == \"deberta\":\n        name = \"microsoft/deberta-v3-base\"\n    elif model_name==\"contrastive1\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_1\"\n    elif model_name==\"contrastive2\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_2\"\n    elif model_name==\"contrastive3\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_3\"\n    elif model_name==\"contrastive4\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_4\"\n    elif model_name==\"contrastive5\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_5\"\n    elif model_name==\"contrastive6\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_6\"\n    elif model_name==\"contrastive7\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_7\"\n    elif model_name==\"contrastive8\":\n        name=\"/kaggle/working/contrastive_encoders/prompt_8\"\n    elif model_name==\"contrastive\":\n        name=\"/kaggle/working/contrastive_encoders/final_encoder\"\n        \n    else:\n        raise ValueError(\"Unknown model\")\n    \n    tokenizer = AutoTokenizer.from_pretrained(name)\n    encoder = AutoModel.from_pretrained(name)\n    hidden_size = encoder.config.hidden_size\n\n    # Freeze all params\n    #for param in encoder.parameters():\n        #param.requires_grad = False\n\n    # Unfreeze last `unfreeze_last_n` layers if requested\n    #if hasattr(encoder, \"encoder\"):  # works for RoBERTa/E5/BGE\n    layers = encoder.encoder.layer\n    for layer in layers[-unfreeze_last_n:]:\n        for param in layer.parameters():\n            param.requires_grad = True\n    #elif hasattr(encoder, \"model\"):  # some Qwen variants wrap transformer under .model\n        #if hasattr(encoder.model, \"layers\"):\n            #layers = encoder.model.layers\n            #for layer in layers[-unfreeze_last_n:]:\n                #for param in layer.parameters():\n                   # param.requires_grad = True\n\n    return encoder, tokenizer, hidden_size\n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T23:48:59.242507Z","iopub.execute_input":"2025-09-12T23:48:59.242837Z","iopub.status.idle":"2025-09-12T23:48:59.250562Z","shell.execute_reply.started":"2025-09-12T23:48:59.242792Z","shell.execute_reply":"2025-09-12T23:48:59.249736Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"file_path = '/kaggle/input/asapaes/training_set_rel3_cleaned.tsv'\ncolumns = ['essay_id', 'essay_set', 'essay', 'domain1_score']\nasap = pd.read_csv(file_path, sep='\\t', encoding='ISO-8859-1', usecols=columns)\nmin_scores = [int(asap[asap[\"essay_set\"] == s][\"domain1_score\"].min()) for s in range(1, 9)]\nmax_scores = [int(asap[asap[\"essay_set\"] == s][\"domain1_score\"].max()) for s in range(1, 9)]\nasap.head()","metadata":{"execution":{"iopub.status.busy":"2025-09-12T20:45:08.551677Z","iopub.execute_input":"2025-09-12T20:45:08.552036Z","iopub.status.idle":"2025-09-12T20:45:09.488534Z","shell.execute_reply.started":"2025-09-12T20:45:08.552012Z","shell.execute_reply":"2025-09-12T20:45:09.487624Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   essay_id  essay_set                                              essay  \\\n0         1          1  Dear local newspaper, I think effects computer...   \n1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4         5          1  Dear @LOCATION1, I know having computers has a...   \n\n   domain1_score  \n0              8  \n1              9  \n2              7  \n3             10  \n4              8  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>domain1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef make_pairs(df, set_id, min_scores, max_scores, threshold=0.1, n_pairs=5000, seed=42):\n    np.random.seed(seed)\n    essays = df[df[\"essay_set\"] == set_id].reset_index(drop=True)\n\n    # normalize scores\n    essays[\"norm_score\"] = normalize_scores(\n        essays[\"domain1_score\"].values, set_id, min_scores, max_scores\n    )\n\n    pairs = []\n    while len(pairs) < n_pairs:\n        i, j = np.random.choice(len(essays), 2, replace=False)\n        s1, s2 = essays.loc[i, \"norm_score\"], essays.loc[j, \"norm_score\"]\n\n        if abs(s1 - s2) <= threshold:\n            pairs.append({\n                \"essay1\": essays.loc[i, \"essay\"],\n                \"essay2\": essays.loc[j, \"essay\"],\n                \"score1\": s1,\n                \"score2\": s2\n            })\n\n    return pd.DataFrame(pairs)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:45:14.276379Z","iopub.execute_input":"2025-09-12T20:45:14.277139Z","iopub.status.idle":"2025-09-12T20:45:14.285704Z","shell.execute_reply.started":"2025-09-12T20:45:14.277103Z","shell.execute_reply":"2025-09-12T20:45:14.284827Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\n\nclass EssayPairDataset(Dataset):\n    def __init__(self, pairs, tokenizer, max_len=256):\n        self.pairs = pairs\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        row = self.pairs.iloc[idx]\n\n        enc1 = self.tokenizer(\n            row[\"essay1\"], truncation=True, padding=\"max_length\",\n            max_length=self.max_len, return_tensors=\"pt\"\n        )\n        enc2 = self.tokenizer(\n            row[\"essay2\"], truncation=True, padding=\"max_length\",\n            max_length=self.max_len, return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids1\": enc1[\"input_ids\"].squeeze(0),\n            \"attention_mask1\": enc1[\"attention_mask\"].squeeze(0),\n            \"input_ids2\": enc2[\"input_ids\"].squeeze(0),\n            \"attention_mask2\": enc2[\"attention_mask\"].squeeze(0),\n            \"score1\": torch.tensor(row[\"score1\"], dtype=torch.float),\n            \"score2\": torch.tensor(row[\"score2\"], dtype=torch.float)\n        }\n\n\n\ndef get_pair_dataloader(pairs, tokenizer, batch_size=16, max_len=256, shuffle=True):\n    dataset = EssayPairDataset(pairs, tokenizer, max_len=max_len)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:45:18.765858Z","iopub.execute_input":"2025-09-12T20:45:18.766139Z","iopub.status.idle":"2025-09-12T20:45:18.773619Z","shell.execute_reply.started":"2025-09-12T20:45:18.766116Z","shell.execute_reply":"2025-09-12T20:45:18.772912Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\n\nclass ContrastiveModel(nn.Module):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder  # pretrained LM\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS]\n        return cls_embedding\n\n\ndef info_nce_loss(emb1, emb2, labels, temperature=0.07, threshold=0.1):\n    \"\"\"\n    Supervised InfoNCE loss for AES.\n    \n    emb1, emb2: [batch, hidden_dim]\n    labels: [batch] normalized scores (0-1 per essay)\n    threshold: max difference in scores to consider essays as 'similar'\n    \"\"\"\n    # normalize embeddings\n    emb1 = F.normalize(emb1, dim=-1)\n    emb2 = F.normalize(emb2, dim=-1)\n\n    # similarity matrix (cosine)\n    logits = torch.matmul(emb1, emb2.T) / temperature  # [batch, batch]\n\n    # expand labels to pairwise matrix\n    diff = torch.abs(labels.unsqueeze(1) - labels.unsqueeze(0))  # [batch, batch]\n\n    # positive mask: 1 if within threshold, 0 otherwise\n    positive_mask = (diff <= threshold).float()\n    # remove self-similarity\n    positive_mask.fill_diagonal_(0)\n\n    # log-softmax over rows\n    log_probs = F.log_softmax(logits, dim=1)\n\n    # supervised contrastive loss: average log-prob over positives\n    numerator = (positive_mask * log_probs).sum(dim=1)\n    denominator = positive_mask.sum(dim=1).clamp(min=1)  # avoid divide by zero\n    loss = -(numerator / denominator).mean()\n\n    return loss\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:45:24.802918Z","iopub.execute_input":"2025-09-12T20:45:24.803728Z","iopub.status.idle":"2025-09-12T20:45:24.810134Z","shell.execute_reply.started":"2025-09-12T20:45:24.803703Z","shell.execute_reply":"2025-09-12T20:45:24.809242Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train_contrastive(encoder, pair_loader, optimizer, device, num_epochs=3):\n    encoder.train()\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        for batch in tqdm(pair_loader, desc=f\"Contrastive Epoch {epoch+1}\"):\n            input1 = {\n                \"input_ids\": batch[\"input_ids1\"].to(device),\n                \"attention_mask\": batch[\"attention_mask1\"].to(device)\n            }\n            input2 = {\n                \"input_ids\": batch[\"input_ids2\"].to(device),\n                \"attention_mask\": batch[\"attention_mask2\"].to(device)\n            }\n            labels = torch.stack([batch[\"score1\"], batch[\"score2\"]], dim=1).to(device)\n\n            emb1 = encoder(**input1).last_hidden_state[:, 0, :]\n            emb2 = encoder(**input2).last_hidden_state[:, 0, :]\n\n            # Use score1 as \"labels\" (since each emb1 corresponds to essay1)\n            loss = info_nce_loss(emb1, emb2, batch[\"score1\"].to(device))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(pair_loader)\n        print(f\"Epoch {epoch+1}, Avg Loss={avg_loss:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:45:28.928427Z","iopub.execute_input":"2025-09-12T20:45:28.928709Z","iopub.status.idle":"2025-09-12T20:45:28.935630Z","shell.execute_reply.started":"2025-09-12T20:45:28.928690Z","shell.execute_reply":"2025-09-12T20:45:28.934818Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nencoder = AutoModel.from_pretrained(\"roberta-base\").to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\noptimizer = torch.optim.AdamW(encoder.parameters(), lr=2e-5)\n\n\n\nsave_dir = \"contrastive_encoders\"\nos.makedirs(save_dir, exist_ok=True)\n\nfor set_id in range(1, 9):\n    print(f\"\\n=== Training on Prompt {set_id} ===\")\n\n    # make pairs for this prompt\n    pairs_df = make_pairs(\n        df=asap,\n        set_id=set_id,\n        min_scores=min_scores,\n        max_scores=max_scores,\n        threshold=0.1,\n        n_pairs=3000\n    )\n\n    # dataloader\n    pair_loader = get_pair_dataloader(\n        pairs=pairs_df,\n        tokenizer=tokenizer,\n        batch_size=16,\n        max_len=256\n    )\n\n    # train\n    train_contrastive(\n        encoder=encoder,\n        pair_loader=pair_loader,\n        optimizer=optimizer,\n        device=device,\n        num_epochs=3\n    )\n\n    # save encoder after each prompt\n    encoder.save_pretrained(f\"{save_dir}/prompt_{set_id}\")\n    tokenizer.save_pretrained(f\"{save_dir}/prompt_{set_id}\")\n\n# final save (after all prompts)\nencoder.save_pretrained(f\"{save_dir}/final_encoder\")\ntokenizer.save_pretrained(f\"{save_dir}/final_encoder\")\nprint(\"\\n✅ Encoder saved successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:45:32.977297Z","iopub.execute_input":"2025-09-12T20:45:32.977576Z","iopub.status.idle":"2025-09-12T22:45:50.632641Z","shell.execute_reply.started":"2025-09-12T20:45:32.977555Z","shell.execute_reply":"2025-09-12T22:45:50.632011Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e66f540970146c196a57231490021de"}},"metadata":{}},{"name":"stderr","text":"2025-09-12 20:45:46.953094: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757709947.333079      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757709947.440803      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a79be2f194949fb9fc38abfd49ca33f"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26ef65759ef413aaa5fc2e77345eb92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a9f73b801cd445eb1e2c3c89a507efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bd63468cca04253932705642793d3f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"479608b23023431b8d993096554ba164"}},"metadata":{}},{"name":"stdout","text":"\n=== Training on Prompt 1 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [04:55<00:00,  1.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.6636\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [05:08<00:00,  1.64s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.6516\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [05:08<00:00,  1.64s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=2.6399\n\n=== Training on Prompt 2 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [05:10<00:00,  1.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.6356\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [05:10<00:00,  1.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.3179\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [05:09<00:00,  1.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=2.1268\n\n=== Training on Prompt 3 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [04:48<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.6275\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [04:49<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.4496\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [04:48<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=2.1484\n\n=== Training on Prompt 4 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [04:46<00:00,  1.52s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.3878\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [04:46<00:00,  1.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.1308\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [04:47<00:00,  1.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=2.0092\n\n=== Training on Prompt 5 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [04:51<00:00,  1.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.4524\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [04:50<00:00,  1.55s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.1837\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [04:50<00:00,  1.54s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=1.9856\n\n=== Training on Prompt 6 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [04:55<00:00,  1.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.5972\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [04:56<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.4686\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [04:55<00:00,  1.57s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=2.3506\n\n=== Training on Prompt 7 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [04:56<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.6201\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [04:56<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.5234\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [04:57<00:00,  1.58s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=2.4249\n\n=== Training on Prompt 8 ===\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 1: 100%|██████████| 188/188 [05:15<00:00,  1.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Avg Loss=2.7366\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 2: 100%|██████████| 188/188 [05:15<00:00,  1.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Avg Loss=2.6920\n","output_type":"stream"},{"name":"stderr","text":"Contrastive Epoch 3: 100%|██████████| 188/188 [05:15<00:00,  1.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Avg Loss=2.6666\n\n✅ Encoder saved successfully!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class EssayDataset(Dataset):\n    def __init__(self, texts, scores, tokenizer, max_len=512, embedder_name=None):\n        self.texts = texts\n        self.scores = scores\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.embedder_name = embedder_name  # keep track of which model we use\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        score = self.scores[idx]\n\n        # If using E5, prepend \"passage: \" to the essay\n        if self.embedder_name and \"e5\" in self.embedder_name.lower():\n            text = \"passage: \" + text\n\n        enc = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"score\": torch.tensor(score, dtype=torch.float)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:03:20.972397Z","iopub.execute_input":"2025-09-12T23:03:20.973010Z","iopub.status.idle":"2025-09-12T23:03:20.978496Z","shell.execute_reply.started":"2025-09-12T23:03:20.972985Z","shell.execute_reply":"2025-09-12T23:03:20.977801Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class EssayRegressor(nn.Module):\n    def __init__(self, encoder, hidden_size):\n        super().__init__()\n        self.encoder = encoder\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_size, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n        cls_embedding = outputs.last_hidden_state[:, 0, :]\n        return self.mlp(cls_embedding)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:03:24.203987Z","iopub.execute_input":"2025-09-12T23:03:24.204271Z","iopub.status.idle":"2025-09-12T23:03:24.209423Z","shell.execute_reply.started":"2025-09-12T23:03:24.204248Z","shell.execute_reply":"2025-09-12T23:03:24.208503Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndef split_dataset(prompt, test_size=0.2, val_size=0.2, seed=42):\n    df_prompt = asap[asap[\"essay_set\"] == prompt].copy()\n\n    train_val, test = train_test_split(df_prompt, test_size=test_size, random_state=seed)\n    train, val = train_test_split(train_val, test_size=val_size/(1-test_size), random_state=seed)\n\n    return train, val, test\n\nfrom torch.utils.data import DataLoader\n\ndef get_dataloaders(train_df, val_df, test_df, tokenizer, prompt,\n                    batch_size=16, max_len=512):\n    \"\"\"\n    Build PyTorch dataloaders for a prompt (expects pandas DataFrames).\n    Scores are normalized with normalize_scores before being passed to the Dataset.\n    \"\"\"\n    # Normalize arrays (vectorized)\n    train_scores_norm = normalize_scores(train_df[\"domain1_score\"].values, prompt, min_scores, max_scores)\n    val_scores_norm   = normalize_scores(val_df[\"domain1_score\"].values,   prompt, min_scores, max_scores)\n    test_scores_norm  = normalize_scores(test_df[\"domain1_score\"].values,  prompt, min_scores, max_scores)\n\n    train_dataset = EssayDataset(train_df[\"essay\"].values, train_scores_norm, tokenizer, max_len=max_len)\n    val_dataset   = EssayDataset(val_df[\"essay\"].values,   val_scores_norm,   tokenizer, max_len=max_len)\n    test_dataset  = EssayDataset(test_df[\"essay\"].values,  test_scores_norm,  tokenizer, max_len=max_len)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n\n    return train_loader, val_loader, test_loader\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:03:26.459826Z","iopub.execute_input":"2025-09-12T23:03:26.460123Z","iopub.status.idle":"2025-09-12T23:03:26.466980Z","shell.execute_reply.started":"2025-09-12T23:03:26.460102Z","shell.execute_reply":"2025-09-12T23:03:26.466395Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndef train_all_prompts(embedder,\n                      prompts=range(1,9),\n                      num_epochs=10,\n                      batch_size=16,\n                      lr=2e-5,\n                      patience=3,\n                      max_len=512,\n                      device=None):\n    \"\"\"\n    Loop over essay sets (prompts) and train separately for each.\n    Saves results to results.csv (train_and_evaluate does that).\n    Returns: pandas DataFrame summarizing returns from results.csv for the embedder run.\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    summary = []\n\n    for prompt in prompts:\n        print(\"\\n\" + \"=\"*30)\n        print(f\" Training Essay Set {prompt} with embedder '{embedder}'\")\n        print(\"=\"*30)\n\n        # 1) get encoder & tokenizer for this embedder\n        encoder, tokenizer, hidden_size = get_encoder(embedder)\n\n        # 2) split dataset for this prompt\n        train_df, val_df, test_df = split_dataset(prompt, test_size=0.2, val_size=0.2, seed=42)\n\n        # 3) dataloaders (scores normalized inside)\n        train_loader, val_loader, test_loader = get_dataloaders(train_df, val_df, test_df,\n                                                                tokenizer, prompt,\n                                                                batch_size=batch_size, max_len=max_len)\n\n        # 4) model, optimizer, criterion\n        model = EssayRegressor(encoder, hidden_size)\n        optimizer = optim.AdamW(model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n\n        # 5) train/evaluate for this prompt\n        train_losses, val_losses, val_qwks = train_and_evaluate(\n            model, train_loader, val_loader, test_loader,\n            optimizer, criterion, device, prompt, embedder,\n            num_epochs, patience\n        )\n\n        # 6) read the last row for this prompt from results.csv (optional) and append to summary\n        try:\n            df_results = pd.read_csv(\"results.csv\")\n            df_prompt = df_results[(df_results[\"embedder\"]==embedder) & (df_results[\"prompt\"]==prompt)]\n            if not df_prompt.empty:\n                row = df_prompt.iloc[-1].to_dict()\n            else:\n                row = {\"embedder\": embedder, \"prompt\": prompt, \"best_val_qwk\": None, \"best_val_mse\": None, \"test_qwk\": None, \"test_mse\": None}\n        except FileNotFoundError:\n            row = {\"embedder\": embedder, \"prompt\": prompt, \"best_val_qwk\": None, \"best_val_mse\": None, \"test_qwk\": None, \"test_mse\": None}\n\n        row[\"train_losses\"] = train_losses\n        row[\"val_losses\"] = val_losses\n        row[\"val_qwks\"] = val_qwks\n        summary.append(row)\n\n    summary_df = pd.DataFrame(summary)\n    return summary_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:03:30.895463Z","iopub.execute_input":"2025-09-12T23:03:30.895725Z","iopub.status.idle":"2025-09-12T23:03:30.905489Z","shell.execute_reply.started":"2025-09-12T23:03:30.895707Z","shell.execute_reply":"2025-09-12T23:03:30.904681Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm import tqdm\n\ndef train_and_evaluate(model, train_loader, val_loader, test_loader, \n                       optimizer, criterion, device, prompt, embedder, \n                       num_epochs=10, patience=3):\n\n    model = model.to(device)\n    best_val_qwk = -1.0\n    best_val_mse = float(\"inf\")\n    patience_counter = 0\n\n    train_losses, val_losses, val_qwks = [], [], []\n\n    for epoch in range(1, num_epochs+1):\n        model.train()\n        running_loss = 0.0\n\n        # Training loop with progress bar\n        progress_bar = tqdm(train_loader, desc=f\"Prompt {prompt} Epoch {epoch}\", leave=False)\n        for batch in progress_bar:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            scores = batch[\"score\"].to(device).unsqueeze(1)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids, attention_mask)\n            loss = criterion(outputs, scores)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_train_loss = running_loss / len(train_loader)\n\n        # Validation\n        val_qwk, val_mse = evaluate(model, val_loader, criterion, prompt, device)\n        train_losses.append(avg_train_loss)\n        val_losses.append(val_mse)\n        val_qwks.append(val_qwk)\n\n        print(f\"Prompt {prompt}, Epoch {epoch}: \"\n              f\"Train Loss={avg_train_loss:.4f}, \"\n              f\"Val QWK={val_qwk:.4f}, Val MSE={val_mse:.4f}\")\n\n        # Early stopping\n        if val_qwk > best_val_qwk:\n            best_val_qwk = val_qwk\n            best_val_mse = val_mse\n            patience_counter = 0\n            torch.save(model.state_dict(), f\"best_model_{embedder}_prompt{prompt}.pt\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load best model before testing\n    model.load_state_dict(torch.load(f\"best_model_{embedder}_prompt{prompt}.pt\"))\n\n    # Final test evaluation\n    test_qwk, test_mse = evaluate(model, test_loader, criterion, prompt, device)\n    print(f\"✅ Prompt {prompt} | Test QWK={test_qwk:.4f}, Test MSE={test_mse:.4f}\")\n\n    # Save results to CSV\n    results_df = pd.DataFrame([{\n        \"embedder\": embedder,\n        \"prompt\": prompt,\n        \"best_val_qwk\": best_val_qwk,\n        \"best_val_mse\": best_val_mse,\n        \"test_qwk\": test_qwk,\n        \"test_mse\": test_mse\n    }])\n\n    results_df.to_csv(\"results.csv\", mode=\"a\", header=not os.path.exists(\"results.csv\"), index=False)\n\n    return train_losses, val_losses, val_qwks\n","metadata":{"execution":{"iopub.status.busy":"2025-09-12T23:03:40.652265Z","iopub.execute_input":"2025-09-12T23:03:40.652581Z","iopub.status.idle":"2025-09-12T23:03:40.663388Z","shell.execute_reply.started":"2025-09-12T23:03:40.652559Z","shell.execute_reply":"2025-09-12T23:03:40.662577Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score, mean_squared_error\nimport numpy as np\n\ndef evaluate(model, loader, criterion, prompt, device):\n    \"\"\"\n    Evaluate model on loader.\n    Returns: (qwk_on_raw_scale, mse_on_raw_scale)\n    Note: loader should supply normalized scores (as used during training).\n    \"\"\"\n    model.eval()\n    all_preds_norm, all_labels_norm = [], []\n    total_loss = 0.0\n    n_batches = 0\n\n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            scores = batch[\"score\"].to(device).unsqueeze(1)    # normalized scores\n\n            outputs = model(input_ids, attention_mask)        # normalized-pred outputs\n            loss = criterion(outputs, scores)\n            total_loss += loss.item()\n            n_batches += 1\n\n            # Collect as numpy arrays\n            all_preds_norm.extend(outputs.squeeze(1).cpu().numpy())\n            all_labels_norm.extend(scores.squeeze(1).cpu().numpy())\n\n    if len(all_preds_norm) == 0:\n        return 0.0, 0.0  # safe fallback\n\n    # Convert to numpy arrays\n    all_preds_norm = np.array(all_preds_norm, dtype=float).flatten()\n    all_labels_norm = np.array(all_labels_norm, dtype=float).flatten()\n\n    # Denormalize both (raw score scale)\n    preds_raw = denormalize_scores(all_preds_norm, prompt, min_scores, max_scores)\n    labels_raw = denormalize_scores(all_labels_norm, prompt, min_scores, max_scores)\n\n    # Round predictions to nearest integer and clip to valid range for QWK\n    low, high = min_scores[prompt-1], max_scores[prompt-1]\n    preds_rounded = np.clip(np.rint(preds_raw), low, high).astype(int)\n    labels_int = labels_raw.astype(int)\n\n    # QWK (on integer original-score scale) and raw-scale MSE\n    try:\n        qwk = cohen_kappa_score(labels_int, preds_rounded, weights=\"quadratic\")\n    except Exception:\n        qwk = 0.0\n\n    mse_raw = mean_squared_error(labels_raw, preds_raw)\n\n    avg_loss = total_loss / n_batches if n_batches > 0 else 0.0\n    return qwk, mse_raw\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:03:45.033200Z","iopub.execute_input":"2025-09-12T23:03:45.033950Z","iopub.status.idle":"2025-09-12T23:03:45.046423Z","shell.execute_reply.started":"2025-09-12T23:03:45.033911Z","shell.execute_reply":"2025-09-12T23:03:45.045665Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_bge= train_all_prompts(\n    embedder=\"bge\",\n    prompts=range(1,9),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_bge[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_e5base = train_all_prompts(\n    embedder=\"e5-base\",\n    prompts=range(1,9),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_e5base[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"deberta\",\n    prompts=range(1,9),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive\",\n    prompts=range(1,9),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:05:28.225343Z","iopub.execute_input":"2025-09-12T23:05:28.225914Z","iopub.status.idle":"2025-09-12T23:34:41.550409Z","shell.execute_reply.started":"2025-09-12T23:05:28.225890Z","shell.execute_reply":"2025-09-12T23:34:41.549673Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 1 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 1: Train Loss=0.0725, Val QWK=0.5919, Val MSE=1.2899\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 2: Train Loss=0.0320, Val QWK=0.6287, Val MSE=1.3046\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 3: Train Loss=0.0335, Val QWK=0.6099, Val MSE=1.2648\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 4: Train Loss=0.0333, Val QWK=0.5970, Val MSE=1.3672\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 5: Train Loss=0.0311, Val QWK=0.6166, Val MSE=1.4653\nEarly stopping triggered.\n✅ Prompt 1 | Test QWK=0.6035, Test MSE=1.2566\n\n==============================\n Training Essay Set 2 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 1: Train Loss=0.0609, Val QWK=0.7411, Val MSE=0.2400\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 2: Train Loss=0.0271, Val QWK=0.7286, Val MSE=0.2378\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 3: Train Loss=0.0240, Val QWK=0.7393, Val MSE=0.2691\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 4: Train Loss=0.0225, Val QWK=0.7488, Val MSE=0.2451\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 5: Train Loss=0.0220, Val QWK=0.7066, Val MSE=0.2996\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 6: Train Loss=0.0204, Val QWK=0.7437, Val MSE=0.2561\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 7: Train Loss=0.0208, Val QWK=0.7427, Val MSE=0.2340\nEarly stopping triggered.\n✅ Prompt 2 | Test QWK=0.6824, Test MSE=0.2704\n\n==============================\n Training Essay Set 3 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 1: Train Loss=0.1089, Val QWK=0.6744, Val MSE=0.2719\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 2: Train Loss=0.0567, Val QWK=0.7165, Val MSE=0.2557\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 3: Train Loss=0.0552, Val QWK=0.7044, Val MSE=0.2587\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 4: Train Loss=0.0594, Val QWK=0.7167, Val MSE=0.2494\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 5: Train Loss=0.0533, Val QWK=0.7048, Val MSE=0.2541\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 6: Train Loss=0.0494, Val QWK=0.7237, Val MSE=0.2440\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 7: Train Loss=0.0490, Val QWK=0.7330, Val MSE=0.2418\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 8: Train Loss=0.0524, Val QWK=0.7342, Val MSE=0.2412\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 9: Train Loss=0.0453, Val QWK=0.7439, Val MSE=0.2371\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 10: Train Loss=0.0463, Val QWK=0.7196, Val MSE=0.2353\n✅ Prompt 3 | Test QWK=0.7080, Test MSE=0.2781\n\n==============================\n Training Essay Set 4 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 1: Train Loss=0.1211, Val QWK=0.6828, Val MSE=0.3418\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 2: Train Loss=0.0535, Val QWK=0.7762, Val MSE=0.3057\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 3: Train Loss=0.0508, Val QWK=0.7652, Val MSE=0.3067\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 4: Train Loss=0.0484, Val QWK=0.7829, Val MSE=0.2913\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 5: Train Loss=0.0455, Val QWK=0.7851, Val MSE=0.2846\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 6: Train Loss=0.0456, Val QWK=0.7852, Val MSE=0.2825\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 7: Train Loss=0.0461, Val QWK=0.7781, Val MSE=0.2900\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 8: Train Loss=0.0449, Val QWK=0.7764, Val MSE=0.2882\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 9: Train Loss=0.0460, Val QWK=0.7741, Val MSE=0.2663\nEarly stopping triggered.\n✅ Prompt 4 | Test QWK=0.7746, Test MSE=0.3080\n\n==============================\n Training Essay Set 5 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 1: Train Loss=0.0974, Val QWK=0.6632, Val MSE=0.4102\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 2: Train Loss=0.0395, Val QWK=0.6841, Val MSE=0.4638\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 3: Train Loss=0.0359, Val QWK=0.7012, Val MSE=0.4218\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 4: Train Loss=0.0344, Val QWK=0.7405, Val MSE=0.3886\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 5: Train Loss=0.0325, Val QWK=0.7360, Val MSE=0.3439\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 6: Train Loss=0.0337, Val QWK=0.7521, Val MSE=0.3579\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 7: Train Loss=0.0334, Val QWK=0.7534, Val MSE=0.3285\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 8: Train Loss=0.0309, Val QWK=0.7697, Val MSE=0.3136\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 9: Train Loss=0.0290, Val QWK=0.7833, Val MSE=0.3065\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 10: Train Loss=0.0295, Val QWK=0.7775, Val MSE=0.3179\n✅ Prompt 5 | Test QWK=0.7734, Test MSE=0.2902\n\n==============================\n Training Essay Set 6 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 1: Train Loss=0.0722, Val QWK=0.6450, Val MSE=0.4465\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 2: Train Loss=0.0436, Val QWK=0.7190, Val MSE=0.4492\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 3: Train Loss=0.0463, Val QWK=0.7039, Val MSE=0.4562\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 4: Train Loss=0.0407, Val QWK=0.7256, Val MSE=0.4607\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 5: Train Loss=0.0388, Val QWK=0.7055, Val MSE=0.4501\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 6: Train Loss=0.0404, Val QWK=0.7256, Val MSE=0.4098\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 7: Train Loss=0.0379, Val QWK=0.7287, Val MSE=0.4267\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 8: Train Loss=0.0409, Val QWK=0.7354, Val MSE=0.3975\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 9: Train Loss=0.0363, Val QWK=0.7176, Val MSE=0.4219\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 10: Train Loss=0.0348, Val QWK=0.7098, Val MSE=0.4221\n✅ Prompt 6 | Test QWK=0.7672, Test MSE=0.3254\n\n==============================\n Training Essay Set 7 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 1: Train Loss=0.1105, Val QWK=0.7788, Val MSE=6.1867\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 2: Train Loss=0.0310, Val QWK=0.8170, Val MSE=5.9922\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 3: Train Loss=0.0294, Val QWK=0.8245, Val MSE=5.8221\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 4: Train Loss=0.0275, Val QWK=0.8284, Val MSE=6.2256\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 5: Train Loss=0.0283, Val QWK=0.8244, Val MSE=5.8759\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 6: Train Loss=0.0278, Val QWK=0.8347, Val MSE=5.6759\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 7: Train Loss=0.0257, Val QWK=0.8307, Val MSE=5.7506\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 8: Train Loss=0.0234, Val QWK=0.8376, Val MSE=5.4858\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 9: Train Loss=0.0264, Val QWK=0.8324, Val MSE=5.9653\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 10: Train Loss=0.0220, Val QWK=0.8388, Val MSE=5.4701\n✅ Prompt 7 | Test QWK=0.8916, Test MSE=4.4838\n\n==============================\n Training Essay Set 8 with embedder 'contrastive'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 1: Train Loss=0.0596, Val QWK=0.7813, Val MSE=13.2335\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 2: Train Loss=0.0228, Val QWK=0.8552, Val MSE=12.1525\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 3: Train Loss=0.0195, Val QWK=0.8626, Val MSE=11.3201\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 4: Train Loss=0.0172, Val QWK=0.8466, Val MSE=11.1525\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 5: Train Loss=0.0177, Val QWK=0.8702, Val MSE=11.2592\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 6: Train Loss=0.0197, Val QWK=0.8736, Val MSE=11.4380\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 7: Train Loss=0.0165, Val QWK=0.8775, Val MSE=11.8520\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 8: Train Loss=0.0168, Val QWK=0.8823, Val MSE=10.1371\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 9: Train Loss=0.0160, Val QWK=0.8769, Val MSE=9.7714\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 10: Train Loss=0.0151, Val QWK=0.8875, Val MSE=9.9836\n✅ Prompt 8 | Test QWK=0.8834, Test MSE=6.6007\n      embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive       1      0.628725  0.603490  1.256645\n1  contrastive       2      0.748780  0.682426  0.270378\n2  contrastive       3      0.743886  0.707992  0.278055\n3  contrastive       4      0.785219  0.774605  0.308026\n4  contrastive       5      0.783263  0.773410  0.290208\n5  contrastive       6      0.735418  0.767184  0.325438\n6  contrastive       7      0.838842  0.891631  4.483780\n7  contrastive       8      0.887484  0.883411  6.600673\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive1\",\n    prompts=range(1,2),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:49:11.833013Z","iopub.execute_input":"2025-09-12T23:49:11.833287Z","iopub.status.idle":"2025-09-12T23:57:03.338164Z","shell.execute_reply.started":"2025-09-12T23:49:11.833265Z","shell.execute_reply":"2025-09-12T23:57:03.337452Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 1 with embedder 'contrastive1'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 1: Train Loss=0.0369, Val QWK=0.6480, Val MSE=1.0220\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 2: Train Loss=0.0182, Val QWK=0.6922, Val MSE=0.9805\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 3: Train Loss=0.0128, Val QWK=0.7172, Val MSE=0.9133\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 4: Train Loss=0.0102, Val QWK=0.7998, Val MSE=0.8141\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 5: Train Loss=0.0083, Val QWK=0.6203, Val MSE=1.6266\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 6: Train Loss=0.0080, Val QWK=0.6820, Val MSE=1.0902\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 1, Epoch 7: Train Loss=0.0080, Val QWK=0.6565, Val MSE=1.2488\nEarly stopping triggered.\n✅ Prompt 1 | Test QWK=0.7849, Test MSE=0.7644\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive1       1      0.799799  0.784902  0.764367\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive2\",\n    prompts=range(2,3),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T23:57:45.905705Z","iopub.execute_input":"2025-09-12T23:57:45.906528Z","iopub.status.idle":"2025-09-13T00:06:50.094829Z","shell.execute_reply.started":"2025-09-12T23:57:45.906500Z","shell.execute_reply":"2025-09-13T00:06:50.093979Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 2 with embedder 'contrastive2'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 1: Train Loss=0.0219, Val QWK=0.8249, Val MSE=0.2077\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 2: Train Loss=0.0118, Val QWK=0.8636, Val MSE=0.1664\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 3: Train Loss=0.0080, Val QWK=0.8426, Val MSE=0.1849\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 4: Train Loss=0.0074, Val QWK=0.8740, Val MSE=0.1576\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 5: Train Loss=0.0070, Val QWK=0.8770, Val MSE=0.1388\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 6: Train Loss=0.0057, Val QWK=0.8374, Val MSE=0.1884\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 7: Train Loss=0.0048, Val QWK=0.8697, Val MSE=0.1878\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 2, Epoch 8: Train Loss=0.0044, Val QWK=0.8732, Val MSE=0.1491\nEarly stopping triggered.\n✅ Prompt 2 | Test QWK=0.8522, Test MSE=0.1447\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive2       2      0.876996  0.852208  0.144653\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive3\",\n    prompts=range(3,4),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T00:13:35.399079Z","iopub.execute_input":"2025-09-13T00:13:35.399916Z","iopub.status.idle":"2025-09-13T00:18:48.589669Z","shell.execute_reply.started":"2025-09-13T00:13:35.399891Z","shell.execute_reply":"2025-09-13T00:18:48.588676Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 3 with embedder 'contrastive3'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 1: Train Loss=0.0444, Val QWK=0.9024, Val MSE=0.1264\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 2: Train Loss=0.0222, Val QWK=0.9134, Val MSE=0.1019\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 3: Train Loss=0.0160, Val QWK=0.9121, Val MSE=0.1043\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 4: Train Loss=0.0112, Val QWK=0.9123, Val MSE=0.1031\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 3, Epoch 5: Train Loss=0.0099, Val QWK=0.8993, Val MSE=0.1349\nEarly stopping triggered.\n✅ Prompt 3 | Test QWK=0.9342, Test MSE=0.0868\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive3       3      0.913356  0.934164  0.086796\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive4\",\n    prompts=range(4,5),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T00:18:59.226058Z","iopub.execute_input":"2025-09-13T00:18:59.226911Z","iopub.status.idle":"2025-09-13T00:23:15.396229Z","shell.execute_reply.started":"2025-09-13T00:18:59.226881Z","shell.execute_reply":"2025-09-13T00:23:15.395472Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 4 with embedder 'contrastive4'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 1: Train Loss=0.0275, Val QWK=0.9341, Val MSE=0.1044\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 2: Train Loss=0.0160, Val QWK=0.9128, Val MSE=0.1258\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 3: Train Loss=0.0119, Val QWK=0.9215, Val MSE=0.1384\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 4, Epoch 4: Train Loss=0.0083, Val QWK=0.9116, Val MSE=0.1196\nEarly stopping triggered.\n✅ Prompt 4 | Test QWK=0.9021, Test MSE=0.1398\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive4       4      0.934116  0.902087  0.139788\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive5\",\n    prompts=range(5,6),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T00:25:49.525658Z","iopub.execute_input":"2025-09-13T00:25:49.526177Z","iopub.status.idle":"2025-09-13T00:31:15.954364Z","shell.execute_reply.started":"2025-09-13T00:25:49.526153Z","shell.execute_reply":"2025-09-13T00:31:15.953621Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 5 with embedder 'contrastive5'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 1: Train Loss=0.0322, Val QWK=0.9133, Val MSE=0.1596\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 2: Train Loss=0.0118, Val QWK=0.9396, Val MSE=0.1022\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 3: Train Loss=0.0091, Val QWK=0.9237, Val MSE=0.1328\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 4: Train Loss=0.0073, Val QWK=0.8541, Val MSE=0.2532\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 5, Epoch 5: Train Loss=0.0072, Val QWK=0.8956, Val MSE=0.2073\nEarly stopping triggered.\n✅ Prompt 5 | Test QWK=0.9268, Test MSE=0.0992\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive5       5      0.939646  0.926842  0.099158\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive6\",\n    prompts=range(6,7),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T00:35:04.486583Z","iopub.execute_input":"2025-09-13T00:35:04.486996Z","iopub.status.idle":"2025-09-13T00:45:58.665355Z","shell.execute_reply.started":"2025-09-13T00:35:04.486974Z","shell.execute_reply":"2025-09-13T00:45:58.664466Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 6 with embedder 'contrastive6'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 1: Train Loss=0.0464, Val QWK=0.8469, Val MSE=0.2192\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 2: Train Loss=0.0208, Val QWK=0.7457, Val MSE=0.3672\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 3: Train Loss=0.0154, Val QWK=0.8627, Val MSE=0.2222\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 4: Train Loss=0.0128, Val QWK=0.8643, Val MSE=0.2216\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 5: Train Loss=0.0100, Val QWK=0.8724, Val MSE=0.1930\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 6: Train Loss=0.0100, Val QWK=0.8729, Val MSE=0.2042\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 7: Train Loss=0.0079, Val QWK=0.8670, Val MSE=0.3158\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 8: Train Loss=0.0076, Val QWK=0.8808, Val MSE=0.1984\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 9: Train Loss=0.0062, Val QWK=0.8625, Val MSE=0.2452\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 6, Epoch 10: Train Loss=0.0071, Val QWK=0.8748, Val MSE=0.1834\n✅ Prompt 6 | Test QWK=0.8739, Test MSE=0.1863\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive6       6      0.880808  0.873887   0.18626\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive7\",\n    prompts=range(7,8),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T00:46:46.288556Z","iopub.execute_input":"2025-09-13T00:46:46.289434Z","iopub.status.idle":"2025-09-13T00:56:18.203972Z","shell.execute_reply.started":"2025-09-13T00:46:46.289399Z","shell.execute_reply":"2025-09-13T00:56:18.203065Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 7 with embedder 'contrastive7'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 1: Train Loss=0.0377, Val QWK=0.7709, Val MSE=11.8826\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 2: Train Loss=0.0138, Val QWK=0.8895, Val MSE=4.6976\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 3: Train Loss=0.0110, Val QWK=0.8181, Val MSE=5.0987\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 4: Train Loss=0.0090, Val QWK=0.8961, Val MSE=3.1969\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 5: Train Loss=0.0072, Val QWK=0.9027, Val MSE=2.8713\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 6: Train Loss=0.0058, Val QWK=0.9027, Val MSE=3.6051\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 7: Train Loss=0.0063, Val QWK=0.9027, Val MSE=3.4349\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 8: Train Loss=0.0060, Val QWK=0.9085, Val MSE=3.1186\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 9: Train Loss=0.0051, Val QWK=0.8803, Val MSE=3.4439\n","output_type":"stream"},{"name":"stderr","text":"                                                                                  \r","output_type":"stream"},{"name":"stdout","text":"Prompt 7, Epoch 10: Train Loss=0.0053, Val QWK=0.9045, Val MSE=3.3454\n✅ Prompt 7 | Test QWK=0.9234, Test MSE=2.9050\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive7       7      0.908535  0.923419  2.904962\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# choose device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Example: train Roberta per-prompt\nsummary_deberta = train_all_prompts(\n    embedder=\"contrastive8\",\n    prompts=range(8,9),\n    num_epochs=10,\n    batch_size=8,\n    lr=2e-5,\n    patience=3,\n    max_len=256,\n    device=device\n)\n\n# See summary\nprint(summary_deberta[[\"embedder\",\"prompt\",\"best_val_qwk\",\"test_qwk\",\"test_mse\"]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T00:57:33.195334Z","iopub.execute_input":"2025-09-13T00:57:33.195618Z","iopub.status.idle":"2025-09-13T01:00:53.012939Z","shell.execute_reply.started":"2025-09-13T00:57:33.195595Z","shell.execute_reply":"2025-09-13T01:00:53.011812Z"}},"outputs":[{"name":"stdout","text":"\n==============================\n Training Essay Set 8 with embedder 'contrastive8'\n==============================\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 1: Train Loss=0.0640, Val QWK=0.0543, Val MSE=41.9242\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 2: Train Loss=0.0185, Val QWK=0.5270, Val MSE=28.7059\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 3: Train Loss=0.0121, Val QWK=0.7096, Val MSE=23.5106\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 4: Train Loss=0.0097, Val QWK=0.8823, Val MSE=8.1306\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 5: Train Loss=0.0065, Val QWK=0.6627, Val MSE=19.8974\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 6: Train Loss=0.0066, Val QWK=0.7511, Val MSE=15.7626\n","output_type":"stream"},{"name":"stderr","text":"                                                                               \r","output_type":"stream"},{"name":"stdout","text":"Prompt 8, Epoch 7: Train Loss=0.0057, Val QWK=0.8338, Val MSE=10.1755\nEarly stopping triggered.\n✅ Prompt 8 | Test QWK=0.8200, Test MSE=8.3288\n       embedder  prompt  best_val_qwk  test_qwk  test_mse\n0  contrastive8       8      0.882287  0.820044  8.328772\n","output_type":"stream"}],"execution_count":41}]}