{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2667,"databundleVersionId":29898,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning for Automated Essay Scoring","metadata":{}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"Automated essay scoring (AES) is an NLP task that aims to predict the score of an essay based on a certain set of essay quality metrics. The score depends on the grammatical, organizational, and content features of the essays. Human raters establish rubrics and provide scores based on these criteria. However, employing human raters can pose challenges due to the large number of essays to be graded (which slows down the feedback loop) and the inconsistent grades (different raters may assign different scores to the same essay, or a rater may assign different scores to the same essay if evaluated on different days).\n\nAES systems are computer systems that simulate the scoring characteristics of human raters and address the aforementioned problems. There are several models used in AES systems. The most crucial aspect of an AES system is essay representation or encoding. Essay representation involves capturing useful features from the essays that help measure their quality. Manual feature engineering can extract features in the form of lexical, syntactic, or semantic features. This approach has been employed in industrial AES systems. However, such approaches have drawbacks in terms of generalizability and requiring feature engineering tasks.\n\nDeep learning has become a go-to approach for numerous artificial intelligence tasks, consistently achieving outstanding performance results. Deep learning eliminates the need for feature engineering as it learns automatically behind the scenes.\n\nIn this project, I will demonstrate the use of deep learning for automated essay scoring tasks.","metadata":{}},{"cell_type":"markdown","source":"## Implementation","metadata":{}},{"cell_type":"markdown","source":"### Libraries\nAs it is seen in the following code snippet, I imported a number of libraries from <code>PyTorch</code>, <code>sklearn</code> and <code>python (collections)</code>. \n - <code>torch</code> is a deep learning framework that I used it for building, training and testing my models.\n - <code>torchtext</code> is sub-library in PyTorch for text data that I used it to vectorize and tokenize the essays.\n - <code>Pandas</code> is a data manipulation tool which I used it for loading the data from the disk.\n - <code>matplotlib</code> is data visualization library which I used it for generating graphs.\n - <code>numpy</code> is large and multi-dimensional arrays library which I used it for tranforming data into array.\n - <code>scikit-learn</code> is a popular machine learning library which I used it for measuring rater agreement(<code>cohen_kappa_score</code>)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtext.data import get_tokenizer\nfrom torchtext.vocab import vocab\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import Counter, OrderedDict\n\ntokenizer = get_tokenizer('basic_english')","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:12:43.368405Z","iopub.execute_input":"2024-02-12T06:12:43.368878Z","iopub.status.idle":"2024-02-12T06:12:49.825527Z","shell.execute_reply.started":"2024-02-12T06:12:43.368840Z","shell.execute_reply":"2024-02-12T06:12:49.824146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Model\nThe following section shows the design of a multilayer perceptron model. The model has an embedding layer, 2 linear layers, 2 acitivation functions (ReLU and Sigmoid). \n - The embedding layer: <code>nn.Embedding</code> is used to capture semantic and syntactic information from the essays.\n - The linear layers: <code>nn.Linear</code> represents linear transformation. In the model, there are two linear layers. The first linear layer takes an input from the embedding layer with embedding dimension (embedding_dim) size and generate an output with hidden dimension (hidden_dim) size. The second linear layer is used to generate the score (output).\n - Activation function: <code>nn.ReLU</code> and <code>nn.Sigmoid</code> are the activation function used to transform the linear layer into nonlinear. ReLU is applied to the first linear layer whereas sigmoid is applied tothe second linear layer.\n\nThis class has a constructor (<code>__init__</code>) and a forward pass (<code>forward</code>). In the constructor, the functions and the linear layers are setted. In the <code>forward</code> method, the order of computation is defined. The essay input (transformed into numbers) passed through the embedding layer. The intution here is it will capture semantic and syntactic information of the essay. And then, a mean pooling is applied. The first linear layer took the ouput of the averge pooled values and a ReLU actication function is applied over it. Finally, the second linear layer generates a value and adjusted using sigmoid activation function into a score.","metadata":{}},{"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, num_classes)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, essay):\n        embedded = self.embedding(essay)\n        hidden = torch.mean(embedded, dim=1)\n        layer1 = self.linear1(hidden)\n        layer1_relu = self.relu(layer1)\n        layer2 = self.linear2(layer1_relu)\n        output = self.sigmoid(layer2)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:12:54.654426Z","iopub.execute_input":"2024-02-12T06:12:54.655270Z","iopub.status.idle":"2024-02-12T06:12:54.665931Z","shell.execute_reply.started":"2024-02-12T06:12:54.655226Z","shell.execute_reply":"2024-02-12T06:12:54.664011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom Dataset class\nI created a custom data <code>ASAPDataset</code> that takes a list of data and the vocab. This class contains three methods <code>__init__()</code>, <code>__len__()</code>, and <code>__getitem__()</code>. <code>__getitem__()</code> fetchs a sample from asap-aes dataset based on the given index. The <code>Dataset</code> provides a mechanism to load, preprocess, and iterate over the dataset.\n\nIn addition, there is <code>collate_fn</code> function defined to handle padding within the essay vectors. First, the maximum token length is identified and then set all the vectors of the essays to have the same length. The padding is represented using <code>0</code>.","metadata":{}},{"cell_type":"code","source":"class ASAPDataset(Dataset):\n    def __init__(self, data, vocab):\n        self.data = data\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sample = self.data[index]\n        essay = sample[2]\n        essay = self.vocab(tokenizer(essay))\n        score = sample[3]\n        return essay, score\n\ndef collate_fn(batch):\n    essays, scores = zip(*batch)\n    max_length = max([len(entry) for entry in essays])\n    padded_essays = []\n    for tokens in essays:\n        padded_essay = tokens + [0] * (max_length - len(tokens))\n        padded_essays.append(padded_essay)\n    return torch.tensor(padded_essays, dtype=torch.int64), torch.tensor(scores, dtype=torch.int64)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:12:58.520128Z","iopub.execute_input":"2024-02-12T06:12:58.520593Z","iopub.status.idle":"2024-02-12T06:12:58.530860Z","shell.execute_reply.started":"2024-02-12T06:12:58.520560Z","shell.execute_reply":"2024-02-12T06:12:58.529534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The Learning\nIn this section, I defined two functions, <code>training</code> and <code>testing</code>. The training function takes model, optimizer, dataset, and loss function. The model is an instance of the MLP class, optimizer is setted to Adam optimizer, the data is a batched data processd by the <code>DataLoader</code> and the criterion is a mean squared loss (MSE) function.\n\nThe training function is responsible for the learning component of the model. The model takes a batch of essays and produce an output with similar batch size. The output is in the range of 0-1 as it is squashed using <code>sigmoid</code> activation function. By transforming the actual score into the range of 0-1, loss of the model is computed. For transformation of the actual score into 0-1, I employed <code>Min-Max Normalization</code>.\n$$\n    min-max-normalization = \\frac{score - min}{max  - min}\n$$\nwhere score is the essay score, min is the minimum score in the dataset and max is the maximum score in the dataset.\n\nThe other function in this section is, testing function. This function is used to evaluate the performance of the model. To evaluate the model, the output values are transformed into the actual score format. The model is evaluated against minimizing the loss and the agreement of AES system with the human raters. For minimizing the loss, <code>MSELoss</code> from <code>PyTorch</code> is employed.\n$$\n    MSE = \\frac{1}{n}\\sum(output - scores)^2\n$$\nwhere the output is score predicted by the model and scores are actual score from the dataset. The predicted score in both training and testing has a different form. In training phase, the predict score is in the range of 0-1 whereas during testing it is transformed into the range of in the dataset (please check for the actual scores range of the essay in the <code>essay_set</code> variable).\n\nThe other metrics used to measure the performance of the model is the raters' agreement. <code>scikit-learn</code> has an implementation of Cohen's kappa, <code>cohen_kappa_score</code>. This metrics measures the agreement level between raters. The score ranges from -1 to 1, where 1 indicates complete agreement, 0 agreement equivalent to chance and -1 complete disagreement.\n$$\n    k = 1 - \\frac{\\sum W_{i,j}O_{i,j}}{\\sum W_{i,j}E_{i,j}}\n$$\nwhere $O_{i,j}$ is a histogram matrix with the number of predicted labels that have a rating  of $i$ (actual) that received a predicted value $j$, $E_{i,j}$ is a histogram matrix of expected ratings calculated as the outer product between the actual rating's histogram vector of ratings and the predicted rating's histogram vector of ratings.\n$$\n    W_{i, j} = \\frac{(i-j)^2}{(R-1)^2}\n$$\nwhere $W_{i,j}$ is a weight matrix that is calculated based on the difference between actual and predicted values, and $R$ is the rating range.","metadata":{}},{"cell_type":"code","source":"def training(model, optimizer, data, criterion, prompt):\n    model.train()\n    loss = 0.0\n    for (essay, scores) in data:\n        optimizer.zero_grad()\n        \n        output = model(essay)\n        scores = torch.tensor([min_max_normalization(score.item(), prompt) for score in scores], dtype=torch.float32)\n        scores = scores.reshape(-1, 1)\n        loss = criterion(output, scores)\n        loss.backward()\n        optimizer.step()\n\ndef testing(model, data, criterion, prompt):\n    model.eval()\n    total_loss = 0.0\n    scores_4_qwk, output_4_qwk = [], []\n    with torch.no_grad():\n        for (essay, scores) in data:\n            scores = scores.reshape(-1, 1)\n            output = model(essay)\n            output = torch.tensor([scaler(out.item(), prompt) for out in output ], dtype=torch.float32)\n            output = output.reshape(-1, 1)\n            loss = criterion(output, scores)\n            total_loss += loss\n            \n            scores_4_qwk.append(scores)\n            output_4_qwk.append(output)\n            \n    score_list = [score.item() for tensor_score in scores_4_qwk for score in tensor_score]\n    output_list = [int(output.item()) for tensor_output in output_4_qwk for output in tensor_output]\n    \n    qwk = cohen_kappa_score(score_list, output_list, weights='quadratic')\n    return qwk, total_loss / len(data)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:02.576558Z","iopub.execute_input":"2024-02-12T06:13:02.577031Z","iopub.status.idle":"2024-02-12T06:13:02.590223Z","shell.execute_reply.started":"2024-02-12T06:13:02.576996Z","shell.execute_reply":"2024-02-12T06:13:02.588799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the dataset\nThe ASAP-AES dataset is a popular dataset among AES researchers. The dataset can be downloaded from [Kaggle](https://www.kaggle.com/c/asap-aes). The dataset has 12976 entries and 28 columns (features). But for this project, I am only interested on 4 features, namely essay_id, essay_set, essay, and domain1_score.\n - essay_id is a unique id column for each entry\n - essay_set is an essay category. There are 8 essay sets, and each set represent different questions and different scoring range.\n - essay is a text response to the prompt given by student. This column is importtant feautre in the scoring process.\n - domain1_score is a score column. This field the summation of scores from two raters. In this project, the target value is this field.","metadata":{}},{"cell_type":"code","source":"file_path = '/kaggle/input/asap-aes/training_set_rel3.tsv'\ncolumns = ['essay_id', 'essay_set', 'essay', 'domain1_score']\nasap = pd.read_csv(file_path, sep='\\t', encoding='ISO-8859-1', usecols=columns)\nasap.head()\nasap.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:07.320368Z","iopub.execute_input":"2024-02-12T06:13:07.320835Z","iopub.status.idle":"2024-02-12T06:13:07.731190Z","shell.execute_reply.started":"2024-02-12T06:13:07.320802Z","shell.execute_reply":"2024-02-12T06:13:07.730021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following section contains an essay_set dictionary and two functions. The essay_set dictionary contains the score range of each prompts. For example, essay_set 1 has minimum value of 2 and a maximum value 12. These values are taken from the dataset description. \n\nThe min_max_normalization and scaler functions are used to transform the scores from score range of the dataset into the range of 0-1 and vice versa.","metadata":{}},{"cell_type":"code","source":"essay_set = {\n    1: (2, 12),\n    2: (1, 6),\n    3: (0, 3),\n    4: (0, 3),\n    5: (0, 4),\n    6: (0, 4),\n    7: (0, 30),\n    8: (0, 60)\n}\ndef min_max_normalization(score, prompt):\n    return (score - essay_set[prompt][0]) / (essay_set[prompt][1] - essay_set[prompt][0])\ndef scaler(score, prompt):\n    return round(score * (essay_set[prompt][1] - essay_set[prompt][0]) + essay_set[prompt][0])","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:11.553561Z","iopub.execute_input":"2024-02-12T06:13:11.554476Z","iopub.status.idle":"2024-02-12T06:13:11.562189Z","shell.execute_reply.started":"2024-02-12T06:13:11.554433Z","shell.execute_reply":"2024-02-12T06:13:11.560720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is splited into train, validation and test dataset. For training, 60% of the data is used, 20% of the data is used for validation and the rest is used for testing. <code>random_split</code> from PyTorch is used to spilt the data into the three settings.","metadata":{}},{"cell_type":"code","source":"def split_dataset(prompt): \n    train, val, test = torch.utils.data.random_split(asap[asap['essay_set']==prompt].values, [0.6, 0.2, 0.2], generator=torch.Generator().manual_seed(42))\n    return train, val, test","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:17.595794Z","iopub.execute_input":"2024-02-12T06:13:17.596340Z","iopub.status.idle":"2024-02-12T06:13:17.604150Z","shell.execute_reply.started":"2024-02-12T06:13:17.596302Z","shell.execute_reply":"2024-02-12T06:13:17.602202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text representation\nThe following section shows where the texts are transformed into numbers. Every text in the essay is represented by a number.","metadata":{}},{"cell_type":"code","source":"def essay_vectorizer(text):\n    tokenized_essay = [tokenizer(field[2].lower()) for field in text]\n\n    tokens = set()\n    for essay in tokenized_essay:\n        tokens.update(essay)\n    \n    tokens = list(tokens)\n\n    vocab_essay = vocab(OrderedDict([(token, 1) for token in tokens]), specials=['<unk>'])\n    vocab_essay.set_default_index(vocab_essay['<unk>'])\n\n    return vocab_essay","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:21.033511Z","iopub.execute_input":"2024-02-12T06:13:21.033993Z","iopub.status.idle":"2024-02-12T06:13:21.042037Z","shell.execute_reply.started":"2024-02-12T06:13:21.033953Z","shell.execute_reply":"2024-02-12T06:13:21.040733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def essay_dataloader(prompt, batch_size):\n\n    train, val, test = split_dataset(prompt)\n    vocab_essay = essay_vectorizer(train)\n\n    asap_train = ASAPDataset(train, vocab_essay)\n    train_dl = DataLoader(asap_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\n    asap_val = ASAPDataset(val, vocab_essay)\n    val_dl = DataLoader(asap_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n    asap_test = ASAPDataset(test, vocab_essay)\n    test_dl = DataLoader(asap_test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n    \n    return train_dl, val_dl, test_dl, vocab_essay","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:23.818248Z","iopub.execute_input":"2024-02-12T06:13:23.818727Z","iopub.status.idle":"2024-02-12T06:13:23.826688Z","shell.execute_reply.started":"2024-02-12T06:13:23.818689Z","shell.execute_reply":"2024-02-12T06:13:23.825377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def graphs(result, title, xlabel, ylabel, num_epochs, prompt):\n    epochs = np.arange(1, num_epochs + 1, 1)\n    #result = np.array(result)\n\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    if type(result[0]) == list:\n        for idx, prompt_result in enumerate(result):\n            plt.plot(epochs, np.array(prompt_result), label=f'Prompt {idx + 1}')\n    else:\n        plt.plot(epochs, np.array(result), label=f'Prompt {prompt}')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:26.619276Z","iopub.execute_input":"2024-02-12T06:13:26.620879Z","iopub.status.idle":"2024-02-12T06:13:26.629601Z","shell.execute_reply.started":"2024-02-12T06:13:26.620814Z","shell.execute_reply":"2024-02-12T06:13:26.628445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the ASAP-AES dataset, there are eight prompts. Therefore, we train, validate and test the model using each prompt.  ","metadata":{}},{"cell_type":"code","source":"prompt = 1\nembedding_dim = 50\ncriterion = nn.MSELoss()\nhidden_dim = 100\nEPOCHS = 50\nbatch_size = 32\nnum_classes = 1\n\nqwk_prompts_val, mse_prompts_val, qwk_prompts_test, mse_prompts_test  = [], [], [], []\n\nfor prompt in range(1, 9):\n    train_dl, val_dl, test_dl, vocab_essay = essay_dataloader(prompt, batch_size)\n    vocab_size = len(vocab_essay)\n    \n    model = MLP(vocab_size, embedding_dim, hidden_dim, num_classes)\n    optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n    \n    qwk_epoch, mse_epoch = [], []\n    for epoch in range(0, EPOCHS):\n        training(model, optimizer, train_dl, criterion, prompt) # Training\n        qwk, mse = testing(model, val_dl, criterion, prompt) # Validation\n        \n        qwk_epoch.append(qwk)\n        mse_epoch.append(mse.item())\n        \n    qwk_prompts_val.append(qwk_epoch)\n    mse_prompts_val.append(mse_epoch)\n\n    #Testing\n    qwk_test, mse_test = testing(model, test_dl, criterion, prompt)\n    qwk_prompts_test.append(qwk_test)\n    mse_prompts_test.append(mse_test) ","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:13:30.158099Z","iopub.execute_input":"2024-02-12T06:13:30.158587Z","iopub.status.idle":"2024-02-12T06:16:57.253505Z","shell.execute_reply.started":"2024-02-12T06:13:30.158549Z","shell.execute_reply":"2024-02-12T06:16:57.252091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation visualization\nThe following graphs are the visualization of validation dataset for raters agreement and the errors. The <code>y-axis</code> of <code>Model(Error)</code> shows an error of 0-35. The reason behind this large error value is the output scores are transformed into the original score range of the prompts during validation and testing.","metadata":{}},{"cell_type":"code","source":"graph_settings = {\n    'title': ('Model performance(QWK)', 'Model Error(MSE)'),\n    'xlabel': '# Epochs',\n    'ylabel': ('Agreement (QWK)', 'Error (MSE)')\n}\n\ngraphs(qwk_prompts_val, graph_settings['title'][0], graph_settings['xlabel'], graph_settings['ylabel'][0], EPOCHS, prompt)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:17:13.178509Z","iopub.execute_input":"2024-02-12T06:17:13.179380Z","iopub.status.idle":"2024-02-12T06:17:13.571971Z","shell.execute_reply.started":"2024-02-12T06:17:13.179334Z","shell.execute_reply":"2024-02-12T06:17:13.570628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"graphs(mse_prompts_val, graph_settings['title'][1], graph_settings['xlabel'], graph_settings['ylabel'][1], EPOCHS, prompt)","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:17:19.997270Z","iopub.execute_input":"2024-02-12T06:17:19.997759Z","iopub.status.idle":"2024-02-12T06:17:20.281927Z","shell.execute_reply.started":"2024-02-12T06:17:19.997702Z","shell.execute_reply":"2024-02-12T06:17:20.280959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Testing\nThe following graphs shows the perfomance of the model. The number of epochs  and hidden layers are fixed to 50 and 2 respectively.","metadata":{}},{"cell_type":"code","source":"prompts = range(1, 9)\nplt.bar(prompts, qwk_prompts_test)\nplt.xlabel('Prompts')\nplt.ylabel('Agreement (QWK)')\nplt.title('Model performance (QWK)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-12T06:17:27.030383Z","iopub.execute_input":"2024-02-12T06:17:27.030943Z","iopub.status.idle":"2024-02-12T06:17:27.298291Z","shell.execute_reply.started":"2024-02-12T06:17:27.030900Z","shell.execute_reply":"2024-02-12T06:17:27.297030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary\nIn this project, I demonstrated the use of a multilayer perceptron for automated essay scoring task. The performance varies with the number of epochs and the number of layers.","metadata":{}}]}